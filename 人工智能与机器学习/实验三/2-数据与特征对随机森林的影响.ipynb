{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 更多的数据效果会不会更好呢？\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入工具包\n",
    "import pandas as pd\n",
    "\n",
    "# 读取数据\n",
    "features = pd.read_csv('data/temps_extended.csv')\n",
    "features.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据规模 (2191, 12)\n",
    "\n",
    "新的数据中，数据规模发生了变化，数据量扩充到了2191条并且加入了新的天气指标：\n",
    "\n",
    "* ws_1：前一天的风速\n",
    "* prcp_1: 前一天的降水\n",
    "* snwd_1：前一天的积雪深度\n",
    "\n",
    "既然有了新的特征，先来看看他们长什么样吧，同样的方式绘制就可以了："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('数据规模',features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 统计指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(features.describe(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转换成标准格式\n",
    "import datetime\n",
    "\n",
    "# 得到各种日期数据\n",
    "years = features['year']\n",
    "months = features['month']\n",
    "days = features['day']\n",
    "\n",
    "# 格式转换\n",
    "dates = [str(int(year)) + '-' + str(int(month)) + '-' + str(int(day)) for year, month, day in zip(years, months, days)]\n",
    "dates = [datetime.datetime.strptime(date, '%Y-%m-%d') for date in dates]\n",
    "\n",
    "# 绘图\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# 风格设置\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the plotting layout\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2, figsize = (15,10))\n",
    "fig.autofmt_xdate(rotation = 45)\n",
    "\n",
    "# Actual max temperature measurement\n",
    "ax1.plot(dates, features['actual'])\n",
    "ax1.set_xlabel(''); ax1.set_ylabel('Temperature (F)'); ax1.set_title('Max Temp')\n",
    "\n",
    "# Temperature from 1 day ago\n",
    "ax2.plot(dates, features['temp_1'])\n",
    "ax2.set_xlabel(''); ax2.set_ylabel('Temperature (F)'); ax2.set_title('Prior Max Temp')\n",
    "\n",
    "# Temperature from 2 days ago\n",
    "ax3.plot(dates, features['temp_2'])\n",
    "ax3.set_xlabel('Date'); ax3.set_ylabel('Temperature (F)'); ax3.set_title('Two Days Prior Max Temp')\n",
    "\n",
    "# Friend Estimate\n",
    "ax4.plot(dates, features['friend'])\n",
    "ax4.set_xlabel('Date'); ax4.set_ylabel('Temperature (F)'); ax4.set_title('Friend Estimate')\n",
    "\n",
    "plt.tight_layout(pad=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在数据分析和特征提取的过程中，我们的出发点都是尽可能多的选择有价值的特征，因为其实阶段我们能得到的信息越多，之后建模可以利用的信息也是越多的，比如在这份数据中，我们有完整日期数据，但是显示天气的变换肯定是跟季节因素有关的，但是在原始数据集中并没有体现出季节的指标，我们可以自己创建一个季节变量当做新的特征，无论是对之后建模还是分析都会起到帮助的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置整体布局\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2, figsize = (15,10))\n",
    "fig.autofmt_xdate(rotation = 45)\n",
    "\n",
    "# 平均最高气温\n",
    "ax1.plot(dates, features['average'])\n",
    "ax1.set_xlabel(''); ax1.set_ylabel('Temperature (F)'); ax1.set_title('Historical Avg Max Temp')\n",
    "\n",
    "# 风速\n",
    "ax2.plot(dates, features['ws_1'], 'r-')\n",
    "ax2.set_xlabel(''); ax2.set_ylabel('Wind Speed (mph)'); ax2.set_title('Prior Wind Speed')\n",
    "\n",
    "# 降水\n",
    "ax3.plot(dates, features['prcp_1'], 'r-')\n",
    "ax3.set_xlabel('Date'); ax3.set_ylabel('Precipitation (in)'); ax3.set_title('Prior Precipitation')\n",
    "\n",
    "# 积雪\n",
    "ax4.plot(dates, features['snwd_1'], 'ro')\n",
    "ax4.set_xlabel('Date'); ax4.set_ylabel('Snow Depth (in)'); ax4.set_title('Prior Snow Depth')\n",
    "\n",
    "plt.tight_layout(pad=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有了季节特征之后，假如我想观察一下不同季节的时候上述各项指标的变换情况该怎么做呢？这里给大家推荐一个非常实用的绘图函数pairplot，需要我们先安装seaborn这个工具包，它相当于是在Matplotlib的基础上进行封装，说白了就是用起来更简单规范了："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pairplots\n",
    "\n",
    "最简单实用的！用它来看看这些特征有没有啥用呢！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个季节变量\n",
    "seasons = []\n",
    "\n",
    "for month in features['month']:\n",
    "    if month in [1, 2, 12]:\n",
    "        seasons.append('winter')\n",
    "    elif month in [3, 4, 5]:\n",
    "        seasons.append('spring')\n",
    "    elif month in [6, 7, 8]:\n",
    "        seasons.append('summer')\n",
    "    elif month in [9, 10, 11]:\n",
    "        seasons.append('fall')\n",
    "\n",
    "# 有了季节我们就可以分析更多东西了\n",
    "reduced_features = features[['temp_1', 'prcp_1', 'average', 'actual']]\n",
    "reduced_features['season'] = seasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 导入seaborn工具包\n",
    "import seaborn as sns\n",
    "sns.set(style=\"ticks\", color_codes=True);\n",
    "\n",
    "# 选择你喜欢的颜色模板\n",
    "palette = sns.xkcd_palette(['dark blue', 'dark green', 'gold', 'orange'])\n",
    "\n",
    "# 绘制pairplot\n",
    "sns.pairplot(reduced_features, hue = 'season', diag_kind = 'kde', palette= palette, plot_kws=dict(alpha = 0.7),\n",
    "                   diag_kws=dict(shade=True)); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，x轴和y轴都是我们这4项指标，不同颜色的点表示不同的季节，在主对角线上x轴和y轴都是相同特征表示其在不同季节时的数值分布情况，其他位置用散点图来表示两个特征之间的关系，例如在左下角temp_1和actual就呈现出了很强的相关性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 独热编码\n",
    "features = pd.get_dummies(features)\n",
    "\n",
    "# 提取特征和标签\n",
    "labels = features['actual']\n",
    "features = features.drop('actual', axis = 1)\n",
    "\n",
    "# 特征名字留着备用\n",
    "feature_list = list(features.columns)\n",
    "\n",
    "# 转换成所需格式\n",
    "import numpy as np\n",
    "\n",
    "features = np.array(features)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# 数据集切分\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, \n",
    "                                                                            test_size = 0.25, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training Features Shape:', train_features.shape)\n",
    "print('Training Labels Shape:', train_labels.shape)\n",
    "print('Testing Features Shape:', test_features.shape)\n",
    "print('Testing Labels Shape:', test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "新的训练集由1643个样本组成，测试集有548个样本。先来看看老数据集的情况，这里由于我们重新打开了一个新的notebook，所有代码中重新读取了老温度数据集，并进行了相同的预处理操作："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 先来看看老数据的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 工具包导入\n",
    "import pandas as pd\n",
    "\n",
    "# 为了剔除特征个数对结果的影响，这里特征统一只有老数据集中特征\n",
    "original_feature_indices = [feature_list.index(feature) for feature in\n",
    "                                      feature_list if feature not in\n",
    "                                      ['ws_1', 'prcp_1', 'snwd_1']]\n",
    "\n",
    "# 读取老数据集\n",
    "original_features = pd.read_csv('data/temps.csv')\n",
    "\n",
    "original_features = pd.get_dummies(original_features)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# 数据和标签转换\n",
    "original_labels = np.array(original_features['actual'])\n",
    "\n",
    "original_features= original_features.drop('actual', axis = 1)\n",
    "\n",
    "original_feature_list = list(original_features.columns)\n",
    "\n",
    "original_features = np.array(original_features)\n",
    "\n",
    "# 数据集切分\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "original_train_features, original_test_features, original_train_labels, original_test_labels = train_test_split(original_features, original_labels, test_size = 0.25, random_state = 42)\n",
    "\n",
    "# 同样的树模型进行建模\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# 同样的参数与随机种子\n",
    "rf = RandomForestRegressor(n_estimators= 100, random_state=0)\n",
    "\n",
    "# 这里的训练集使用的是老数据集的\n",
    "rf.fit(original_train_features, original_train_labels);\n",
    "\n",
    "# 为了测试效果能够公平，统一使用一致的测试集，这里选择了刚刚我切分过的新数据集的测试集\n",
    "predictions = rf.predict(test_features[:,original_feature_indices])\n",
    "\n",
    "# 先计算温度平均误差\n",
    "errors = abs(predictions - test_labels)\n",
    "\n",
    "print('平均温度误差:', round(np.mean(errors), 2), 'degrees.')\n",
    "\n",
    "# MAPE\n",
    "mape = 100 * (errors / test_labels)\n",
    "\n",
    "# 这里的Accuracy为了方便观察，我们就用100减去误差了，希望这个值能够越大越好\n",
    "accuracy = 100 - np.mean(mape)\n",
    "print('Accuracy:', round(accuracy, 2), '%.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，当我们把数据量增大之后，效果发生了一些提升，这也符合实际情况，在机器学习任务中，我们都是希望数据量能够越大越好，这样可利用的信息就更多了。\n",
    "下面我们要再对比一下特征数量对结果的影响，之前这两次比较还没有加入新的特征，这回我们把降水，风速，积雪3特征加入训练集中，看看效果又会怎样："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 新数据来了，只增大数据量的话，结果会提升吗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# 剔除掉新的特征，保证数据特征是一致的\n",
    "original_train_features = train_features[:,original_feature_indices]\n",
    "\n",
    "original_test_features = test_features[:, original_feature_indices]\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators= 100 ,random_state=0)\n",
    "\n",
    "rf.fit(original_train_features, train_labels);\n",
    "\n",
    "# 预测\n",
    "baseline_predictions = rf.predict(original_test_features)\n",
    "\n",
    "# 结果\n",
    "baseline_errors = abs(baseline_predictions - test_labels)\n",
    "\n",
    "print('平均温度误差:', round(np.mean(baseline_errors), 2), 'degrees.')\n",
    "\n",
    "# (MAPE)\n",
    "baseline_mape = 100 * np.mean((baseline_errors / test_labels))\n",
    "\n",
    "# accuracy\n",
    "baseline_accuracy = 100 - baseline_mape\n",
    "print('Accuracy:', round(baseline_accuracy, 2), '%.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加入新特征再来看一看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备加入新的特征\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf_exp = RandomForestRegressor(n_estimators= 100, random_state=0)\n",
    "rf_exp.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 同样的测试集\n",
    "predictions = rf_exp.predict(test_features)\n",
    "\n",
    "# 评估\n",
    "errors = abs(predictions - test_labels)\n",
    "\n",
    "print('平均温度误差:', round(np.mean(errors), 2), 'degrees.')\n",
    "\n",
    "# (MAPE)\n",
    "mape = np.mean(100 * (errors / test_labels))\n",
    "\n",
    "# 看一下提升了多少\n",
    "improvement_baseline = 100 * abs(mape - baseline_mape) / baseline_mape\n",
    "print('特征增多后模型效果提升:', round(improvement_baseline, 2), '%.')\n",
    "\n",
    "# accuracy\n",
    "accuracy = 100 - mape\n",
    "print('Accuracy:', round(accuracy, 2), '%.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型整体效果有了略微提升，这里我们还加入一项额外的评估就是模型跟基础模型相比提升的大小，方便来进行对比观察。这回特征也多了，我们可以好好研究下特征重要性这个指标了，虽说其只供参考，但是业界也有一些不成文的行规我们来看一下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征重要性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征名字\n",
    "importances = list(rf_exp.feature_importances_)\n",
    "\n",
    "# 名字，数值组合在一起\n",
    "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
    "\n",
    "# 排序\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "\n",
    "# 打印出来 \n",
    "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指定风格\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# 指定位置\n",
    "x_values = list(range(len(importances)))\n",
    "\n",
    "# 绘图\n",
    "plt.bar(x_values, importances, orientation = 'vertical', color = 'r', edgecolor = 'k', linewidth = 1.2)\n",
    "\n",
    "# x轴名字得竖着写\n",
    "plt.xticks(x_values, feature_list, rotation='vertical')\n",
    "\n",
    "# 图名\n",
    "plt.ylabel('Importance'); plt.xlabel('Variable'); plt.title('Variable Importances');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "之前我们只是简单看了下载特征中哪些更重要，这回我们需要考虑的是特征的累加重要性，先把特征按照其重要性进行排序，再算起累计值，这里用到了cumsum()函数，比如cusm([1,2,3,4])得到的结果就是其累加值(1,3,6,10)，通常我们都以95%为阈值，看看有多少个特征累加在一起之后，其特征重要性的累加值超过该阈值，就取它们当做筛选后的特征："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征重要性累加，看看95%之前有多少个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对特征进行排序\n",
    "sorted_importances = [importance[1] for importance in feature_importances]\n",
    "sorted_features = [importance[0] for importance in feature_importances]\n",
    "\n",
    "# 累计重要性\n",
    "cumulative_importances = np.cumsum(sorted_importances)\n",
    "\n",
    "# 绘制折线图\n",
    "plt.plot(x_values, cumulative_importances, 'g-')\n",
    "\n",
    "# 画一条红色虚线，0.95那\n",
    "plt.hlines(y = 0.95, xmin=0, xmax=len(sorted_importances), color = 'r', linestyles = 'dashed')\n",
    "\n",
    "# X轴\n",
    "plt.xticks(x_values, sorted_features, rotation = 'vertical')\n",
    "\n",
    "# Y轴和名字\n",
    "plt.xlabel('Variable'); plt.ylabel('Cumulative Importance'); plt.title('Cumulative Importances');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里当第5个特征出现的时候，其总体的累加值超过了95%，那么接下来我们的对比实验又来了，如果只用这5个特征效果会怎么样呢？时间效率又会怎样呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 把他们列出来吧！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 看看有几个特征\n",
    "print('Number of features for 95% importance:', np.where(cumulative_importances > 0.95)[0][0] + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练集和测试集要使用一样的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 选择这些特征\n",
    "important_feature_names = [feature[0] for feature in feature_importances[0:5]]\n",
    "# 找到它们的名字\n",
    "important_indices = [feature_list.index(feature) for feature in important_feature_names]\n",
    "\n",
    "# 重新创建训练集\n",
    "important_train_features = train_features[:, important_indices]\n",
    "important_test_features = test_features[:, important_indices]\n",
    "\n",
    "# 数据维度\n",
    "print('Important train features shape:', important_train_features.shape)\n",
    "print('Important test features shape:', important_test_features.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 有这些重要的重新训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 再训练模型\n",
    "rf_exp.fit(important_train_features, train_labels);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 瞅瞅咋样吧！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 同样的测试集\n",
    "predictions = rf_exp.predict(important_test_features)\n",
    "\n",
    "# 评估结果\n",
    "errors = abs(predictions - test_labels)\n",
    "\n",
    "print('平均温度误差:', round(np.mean(errors), 2), 'degrees.')\n",
    "\n",
    "mape = 100 * (errors / test_labels)\n",
    "\n",
    "# accuracy\n",
    "accuracy = 100 - np.mean(mape)\n",
    "print('Accuracy:', round(accuracy, 2), '%.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "效果反而下降了，其实随机森林的算法本身就会考虑特征的问题，会优先选择有价值的，我们认为的去掉一些，相当于可供候选的就少了，出现这样的现象在随机森林中并不奇怪！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算下 Trade-Offs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看起来奇迹并没有出现，本以为效果反而会更好，其实还有一点点下降，这里可能由于是树模型本身具有特征选择的被动技能了。虽然模型没有提升，我们还可以再看看在时间效率的层面上有没有进步呢："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 要计算时间了\n",
    "import time\n",
    "\n",
    "# 这次是用所有特征\n",
    "all_features_time = []\n",
    "\n",
    "# 算一次可能不太准，来10次取个平均\n",
    "for _ in range(10):\n",
    "    start_time = time.time()\n",
    "    rf_exp.fit(train_features, train_labels)\n",
    "    all_features_predictions = rf_exp.predict(test_features)\n",
    "    end_time = time.time()\n",
    "    all_features_time.append(end_time - start_time)\n",
    "\n",
    "all_features_time = np.mean(all_features_time)\n",
    "print('使用所有特征时建模与测试的平均时间消耗:', round(all_features_time, 2), '秒.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当我们使用全部特征的时候，建模与测试用的总时间为0.5秒，这里会由于机器性能导致咱们的速度不一样，大家在笔记本中估计运行时间要比我的稍长一点。再来看看只选择高重要性特征的时间结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这次是用部分重要的特征\n",
    "reduced_features_time = []\n",
    "\n",
    "# 算一次可能不太准，来10次取个平均\n",
    "for _ in range(10):\n",
    "    start_time = time.time()\n",
    "    rf_exp.fit(important_train_features, train_labels)\n",
    "    reduced_features_predictions = rf_exp.predict(important_test_features)\n",
    "    end_time = time.time()\n",
    "    reduced_features_time.append(end_time - start_time)\n",
    "\n",
    "reduced_features_time = np.mean(reduced_features_time)\n",
    "print('使用所有特征时建模与测试的平均时间消耗:', round(reduced_features_time, 2), '秒.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy vs Run-Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用分别的预测值来计算评估结果\n",
    "all_accuracy =  100 * (1- np.mean(abs(all_features_predictions - test_labels) / test_labels))\n",
    "reduced_accuracy = 100 * (1- np.mean(abs(reduced_features_predictions - test_labels) / test_labels))\n",
    "\n",
    "#创建一个df来保存结果\n",
    "comparison = pd.DataFrame({'features': ['all (17)', 'reduced (5)'], \n",
    "                           'run_time': [round(all_features_time, 2), round(reduced_features_time, 2)],\n",
    "                           'accuracy': [round(all_accuracy, 2), round(reduced_accuracy, 2)]})\n",
    "\n",
    "comparison[['features', 'accuracy', 'run_time']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实际中我们也得综合来考虑下性能问题！时间速度别跟我机器比。。。我是专业吃鸡主机！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_accuracy_decrease = 100 * (all_accuracy - reduced_accuracy) / all_accuracy\n",
    "print('相对accuracy下降:', round(relative_accuracy_decrease, 3), '%.')\n",
    "\n",
    "relative_runtime_decrease = 100 * (all_features_time - reduced_features_time) / all_features_time\n",
    "print('相对时间效率提升:', round(relative_runtime_decrease, 3), '%.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "性价比！！！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通常我们买东西都会考虑性价比，这里同样也是这个问题，时间效率的提升相对更大一些，而且基本保证了模型效果是差不多的。\n",
    "最后让我们把所有的实验结果汇总到一起来进行对比吧："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas is used for data manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# Read in data as pandas dataframe and display first 5 rows\n",
    "original_features = pd.read_csv('data/temps.csv')\n",
    "original_features = pd.get_dummies(original_features)\n",
    "\n",
    "# Use numpy to convert to arrays\n",
    "import numpy as np\n",
    "\n",
    "# Labels are the values we want to predict\n",
    "original_labels = np.array(original_features['actual'])\n",
    "\n",
    "# Remove the labels from the features\n",
    "# axis 1 refers to the columns\n",
    "original_features= original_features.drop('actual', axis = 1)\n",
    "\n",
    "# Saving feature names for later use\n",
    "original_feature_list = list(original_features.columns)\n",
    "\n",
    "# Convert to numpy array\n",
    "original_features = np.array(original_features)\n",
    "\n",
    "# Using Skicit-learn to split data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "original_train_features, original_test_features, original_train_labels, original_test_labels = train_test_split(original_features, original_labels, test_size = 0.25, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the original feature indices \n",
    "original_feature_indices = [feature_list.index(feature) for feature in\n",
    "                                      feature_list if feature not in\n",
    "                                      ['ws_1', 'prcp_1', 'snwd_1']]\n",
    "\n",
    "# Create a test set of the original features\n",
    "original_test_features = test_features[:, original_feature_indices]\n",
    "\n",
    "# Time to train on original data set (1 year)\n",
    "original_features_time = []\n",
    "\n",
    "# Do 10 iterations and take average for all features\n",
    "for _ in range(10):\n",
    "    start_time = time.time()\n",
    "    rf.fit(original_train_features, original_train_labels)\n",
    "    original_features_predictions = rf.predict(original_test_features)\n",
    "    end_time = time.time()\n",
    "    original_features_time.append(end_time - start_time)\n",
    "    \n",
    "original_features_time = np.mean(original_features_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean absolute error for each model\n",
    "original_mae = np.mean(abs(original_features_predictions - test_labels))\n",
    "exp_all_mae = np.mean(abs(all_features_predictions - test_labels))\n",
    "exp_reduced_mae = np.mean(abs(reduced_features_predictions - test_labels))\n",
    "\n",
    "# Calculate accuracy for model trained on 1 year of data\n",
    "original_accuracy = 100 * (1 - np.mean(abs(original_features_predictions - test_labels) / test_labels))\n",
    "\n",
    "# Create a dataframe for comparison\n",
    "model_comparison = pd.DataFrame({'model': ['original', 'exp_all', 'exp_reduced'], \n",
    "                                 'error (degrees)':  [original_mae, exp_all_mae, exp_reduced_mae],\n",
    "                                 'accuracy': [original_accuracy, all_accuracy, reduced_accuracy],\n",
    "                                 'run_time (s)': [original_features_time, all_features_time, reduced_features_time]})\n",
    "\n",
    "# Order the dataframe\n",
    "model_comparison = model_comparison[['model', 'error (degrees)', 'accuracy', 'run_time (s)']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘图来总结把\n",
    "# 设置总体布局，还是一整行看起来好一些\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(nrows=1, ncols=3, figsize = (16,5), sharex = True)\n",
    "\n",
    "# X轴\n",
    "x_values = [0, 1, 2]\n",
    "labels = list(model_comparison['model'])\n",
    "plt.xticks(x_values, labels)\n",
    "\n",
    "# 字体大小\n",
    "fontdict = {'fontsize': 18}\n",
    "fontdict_yaxis = {'fontsize': 14}\n",
    "\n",
    "# 预测温度和真实温度差异对比\n",
    "ax1.bar(x_values, model_comparison['error (degrees)'], color = ['b', 'r', 'g'], edgecolor = 'k', linewidth = 1.5)\n",
    "ax1.set_ylim(bottom = 3.5, top = 4.5)\n",
    "ax1.set_ylabel('Error (degrees) (F)', fontdict = fontdict_yaxis); \n",
    "ax1.set_title('Model Error Comparison', fontdict= fontdict)\n",
    "\n",
    "# Accuracy 对比\n",
    "ax2.bar(x_values, model_comparison['accuracy'], color = ['b', 'r', 'g'], edgecolor = 'k', linewidth = 1.5)\n",
    "ax2.set_ylim(bottom = 92, top = 94)\n",
    "ax2.set_ylabel('Accuracy (%)', fontdict = fontdict_yaxis); \n",
    "ax2.set_title('Model Accuracy Comparison', fontdict= fontdict)\n",
    "\n",
    "# 时间效率对比\n",
    "ax3.bar(x_values, model_comparison['run_time (s)'], color = ['b', 'r', 'g'], edgecolor = 'k', linewidth = 1.5)\n",
    "ax3.set_ylim(bottom = 0, top = 1)\n",
    "ax3.set_ylabel('Run Time (sec)', fontdict = fontdict_yaxis); \n",
    "ax3.set_title('Model Run-Time Comparison', fontdict= fontdict);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "original代表是我们的老数据，也就是量少特征少的那份；exp_all代表我们的完整新数据；exp_reduced代表我们按照95%阈值选择的部分重要特征数据集。结果也是很明显的，数据量和特征越多，效果会提升一些，但是时间效率也会有所下降。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
